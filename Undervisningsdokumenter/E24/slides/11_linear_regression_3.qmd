---
format: 
  revealjs:
    theme: css_etc/custom-theme-ilisimatusarfik.css
    slide-level: 0  # Disable automatic slide creation from headings
    incremental: false 
    logo: css_etc/ilisimatusarfik.png
    highlight-style: github-dark
editor: visual
---

::: center
# Line√¶r regression 3

### Statistik E24 (15 ECTS)

### ved Mikkeline Munk Nielsen
:::

------------------------------------------------------------------------

# Regressionsmodellens antagelser

<br>

Antagelser er ***betingelser*** eller ***foruds√¶tninger***, som skal v√¶re opfyldt for, at resultaterne fra regressionen er p√•lidelige og gyldige

<br>

Her ber√∏r vi de mest centrale antagelser bag OLS for dette kursus

------------------------------------------------------------------------

# Regressionsmodellens antagelser

1.  Vores data kommer fra en simpel tilf√¶ldig stikpr√∏ve, der afspejler den population, som vi er interesserede i at unders√∏ge (ellers kan vi ikke arbejde med inferens)

2.  Regressionsmodellen er line√¶r i parametrene og fejlleddet

3.  Ingen af de uafh√¶ngige variable m√• v√¶re en konstant eller line√¶r kombination af de andre uafh√¶ngige variable (dvs. ingen eksakt multikollinaritet)

4.  Residualerne skal v√¶re homogent fordelt (dvs. ingen heteroskedasticitet)

5.  Residualerne skal v√¶re uafh√¶ngigt fordelt (ingen autokorrelation)

6.  Modellen er korrekt specificeret (dvs. alle relevante variable er med (OVB) og ingen irrelevante variable er med

7.  Endogenitet

------------------------------------------------------------------------

# Stikpr√∏ve

**1 Vores data kommer fra en simpel tilf√¶ldig stikpr√∏ve, der afspejler den population, som vi er interesserede i at unders√∏ge (ellers kan vi ikke arbejde med inferens)**

<br>

I statistik drages der konklusioner om en population p√• baggrund af en stikpr√∏ve (inferens). Slutningen er baseret p√•:

**I. Repr√¶sentativitet** (stikpr√∏vens sammens√¶tning ligner populationens mht. relevante karakteristika)

**II. Tilf√¶ldighed** **af stikpr√∏veudtr√¶k** (mindsker risiko for sk√¶vhed i stikpr√∏ven og er derved et middel til at for√∏ge repr√¶sentativitet)

------------------------------------------------------------------------

# Linearitet

**2 Regressionsmodellen er line√¶r i parametrene og fejlleddet**

::::: two-column-layout
::: left-column
-   I line√¶r regression tegnes en lige linje, der beskriver sammenh√¶ngen mellem en afh√¶ngig variabel Y og en (eller flere) uafh√¶ngig variable X

-   H√¶ldningskoefficienterne for den uafh√¶ngige variabel, ùõΩ, repr√¶senterer stigningen i Y, n√•r X stiger med 1

-   Vi antager, at for hver enheds√¶ndring i, vil √¶ndringen i Y v√¶re konstant og proportional til X, og dette forhold er line√¶rt
:::

::: right-column
![](images/h√¶ldning.png)
:::
:::::

------------------------------------------------------------------------

# Multikollinaritet

**3 Ingen af de uafh√¶ngige variable m√• v√¶re en konstant eller line√¶r kombination af de andre uafh√¶ngige variable (dvs. ingen eksakt multikollinaritet)**

<br>

-   Multikollinaritet opst√•r, n√•r der er st√¶rk korrelation mellem uafh√¶ngige variable

$$
cov(X_1, X_2)
$$

-   Som udgangspunkt inkluderer vi kun flere uafh√¶ngige variable i vores model som kontrolvariable, fordi vi mist√¶nker, at der er s√•kaldte ‚Äùconfounders‚Äù. Dermed antager vi ogs√•, at de korrelerer

------------------------------------------------------------------------

# Multikollinaritet

Meget st√¶rk korrelation mellem uafh√¶ngige variable giver dog problemer, da OLS-algoritmen kan have sv√¶rt ved at adskille deres indflydelse p√• den afh√¶ngige variabel, hvilket kan f√∏re til up√•lidelige og inkonsekvente parameterestimater:

<br>

-   St√∏rre standardfejl p√• parameterestimationerne

-   H√∏j $R^2$ p√• trods af insignifikante parameterestimater

-   Sm√• √¶ndringer i data giver store √¶ndringer i parameterestimater

-   Det bliver sv√¶rere at fastsl√•, hvad den enkelte variabel bidrager med til ùëÖ2.

------------------------------------------------------------------------

# Multikollinaritet

**Hvorn√•r oplever man typisk multikollinaritet?**

-   N√¶rt besl√¶gtede variable (f.eks. indkomst og skattebidrag)

-   Polynomier

-   Dummier ‚Äì udelades der ikke en dummy-kategori til som referencekategori opst√•r perfekt multikollinaritet (R vil smide den skyldige variabel ud af modellen)

<br>

**Hvad afsl√∏rer multikollinaritet?**

-   Underlige parameterestimater

-   H√∏j $R^2$ p√• trods af insignifikante t-v√¶rdier

-   Omvendte fortegn p√• parameterestimaterne i forhold til hvad vi teoretisk forventede

------------------------------------------------------------------------

# Multikollinaritet

<br>

Hvad kan vi g√∏re ved multikollinaritet?

<br>

1.  **Dummier**: forhold dig til din referencekategori! Hvilken kategori vil du sammenligne de andre med

2.  **Lav en korrelationsmatrice** over variablene i modellen for at afsl√∏re, hvilke variable der er problemet

3.  **Overvej** om det er meningsfuldt at inkludere variablene p√• en anden m√•de (f.eks. som dummier)

4.  **Eksklud√©r** en variabel fra modellen‚Ä¶

------------------------------------------------------------------------

# Multikollinaritet

```{r include=F}
df <- readRDS("C:/Users/mmn/Dropbox/Ilisimatusarfik/Undervisningsdokumenter/E24/slides/data/ESS.rds") 

library(tidyverse)
df <- df %>% filter(land=="DK") %>% na.omit()

```

Korrelationsmatrice af korrelationskoefficienter mellem variablene net_indomst, kvinde, mand, udd_aar og alder:

```{r echo=TRUE, warning=F, message=F}
library(tidyverse)
corr_df <- df %>% mutate(
  mand = ifelse(koen=="Mand",1,0),
  kvinde = ifelse(koen=="Kvinde",1,0)) %>% 
    select(net_indkomst, kvinde, mand, udd_aar, alder) %>% 
    na.omit()

(corr <- round(cor(corr_df), 1))
```

------------------------------------------------------------------------

# Multikollinaritet

Plot af korrelationsmatrice (St√¶rkere farver indikerer st√¶rkere korrelationer)

```{r echo=TRUE, warning=F, message=F}
library(ggcorrplot)

ggcorrplot(corr)
```

------------------------------------------------------------------------

# Heteroskedasticitet

**4 Residualerne skal v√¶re homogent fordelt (dvs. ingen heteroskedasticitet)**

-   Heteroskedasticitet opst√•r, n√•r der ikke er konstant varians i fejlleddet:

$$
Homoskedasticitet: var(u_i)=\sigma^2
$$

$$
Heteroskedasticitet: var(u_i)=\sigma^2
_i$$

-   Med andre ord beskriver det en situation, hvor variansen p√• fejlleddet er forskellig over forskellige v√¶rdier p√• de uafh√¶ngige variable

-   Man kunne f.eks. forestille sig, at der var st√∏rre variation i indkomst blandt folk mellem 18-28 end blandt folk mellem 38-48

-   Medf√∏rer at parameterestimaters standardafvigelser bliver up√•lidelige, og dermed bliver konfidensintervaller og signifikanstest forfejlede. Det betyder, at vi ikke kan stole p√• vores p-v√¶rdier.

------------------------------------------------------------------------

# Heteroskedasticitet

::::: two-column-layout
::: left-column
-   $\epsilon$ repr√¶senterer fejlleddet eller residualet

-   Fejlleddet er en variabel (ikke et parameter), som indeholder hver respondents afvigelse mellem respondenternes observerede v√¶rdi p√• $Y$ og den v√¶rdi, som regressionsmodellen forudsiger, at respondenten har, p√• baggrund af $\alpha$ og $\beta$

-   $\epsilon$ opsummerer med andre ord den variation i $Y$ (indkomst), som ikke kan forklares af $X$ (uddannelse)
:::

::: right-column
![](images/reg6.png)
:::
:::::

------------------------------------------------------------------------

# Heteroskedasticitet

<br>

-   Man kan inspicere sin model for heteroskedasticitet ved lave et plot, der viser, om residualerne ligger j√¶vnt fordelt over de uafh√¶ngige variable.

<br>

-   Fordi den forudsagte v√¶rdi af den afh√¶ngige variabel ùëåer en funktion af de uafh√¶ngige variable, plotter man tit residualerne mod de forudsagte v√¶rdier som en genvej til at plotte residualerne mod hver enkel uafh√¶ngig variabel

<br>

-   Og hvordan afl√¶ser man s√• s√•dan et plot?

------------------------------------------------------------------------

# Heteroskedasticitet

![](images/HS.png){fig-align="center"}

Kilde: <https://corporatefinanceinstitute.com/resources/data-science/heteroskedasticity/>

------------------------------------------------------------------------

# Heteroskedasticitet

$\hat{indkomst}=\alpha+\beta_1 udd\_aar+\beta_2 antal\_arb\_timer+\epsilon$

::: panel-tabset
### Scatterplot

```{r echo=F, fig.width=6, fig.height=3}
library(ggplot2)

model <- lm(net_indkomst ~ udd_aar+antal_timer_arb, data=df)


# Beregn forudsagte v√¶rdier
predicted <- predict(model)

# Beregn residualer
residuals <- resid(model)

# Lav plot dataframe
plot_data <- data.frame(predicted = predicted, residuals = residuals)

# Opret et plot for heteroskedasticitet
ggplot(plot_data, aes(x = predicted, y = residuals)) +
  geom_point(shape = 20, color = "black") + # S√¶t punktform og farve (runde, sorte punkter)
  labs(
    x = "Forudsagte v√¶rdier",            # Navn til x-aksen
    y = "Residualer",                    # Navn til y-aksen
    title = "Plot for heteroskedasticitet" # Titel til plottet
  ) +
  theme_minimal() +                      # Brug et minimalistisk tema
  theme(
    plot.title = element_text(hjust = 0.5)) # Centr√©r titlen vandret (hjust = 0.5)

```

### Kode

```{r echo=T, eval=F}
library(ggplot2)

model <- lm(net_indkomst ~ udd_aar+antal_timer_arb, data=df)

# Beregn forudsagte v√¶rdier
predicted <- predict(model)

# Beregn residualer
residuals <- resid(model)

# Lav plot dataframe
plot_data <- data.frame(predicted = predicted, residuals = residuals)

# Opret et plot for heteroskedasticitet
ggplot(plot_data, aes(x = predicted, y = residuals)) +
  geom_point(shape = 20, color = "black") + # S√¶t punktform og farve (runde, sorte punkter)
  labs(
    x = "Forudsagte v√¶rdier",            # Navn til x-aksen
    y = "Residualer",                    # Navn til y-aksen
    title = "Plot for heteroskedasticitet" # Titel til plottet
  ) +
  theme_minimal() +                      # Brug et minimalistisk tema
  theme(
    plot.title = element_text(hjust = 0.5)) # Centr√©r titlen vandret (hjust = 0.5)

```
:::

------------------------------------------------------------------------

# Heteroskedasticitet

-   Man kan heldigvis nemt korrigere p√• sine standardfejl, s√• de bliver robuste overfor heteroskedasticitet (det er standardfejlen der bruges til at beregne p-v√¶rdien)

-   Brug f.eks. Bare funktionen `lm_robust()` i R, der fungerer ligesom `lm()`

::: panel-tabset
### Model

```{r echo=F, warning=F, message=F}
library(estimatr)

model_robust <- lm_robust(net_indkomst ~ udd_aar + antal_timer_arb, data = df)

(texreg::screenreg(list(model_robust), include.ci=F,
                custom.coef.names = c("Konstantled",
                                        "Antal √•rs uddannelse",
                                        "Antal arbejdstimer")))
```

### Kode

```{r echo=TRUE, warning=F, message=F, eval=F}
library(estimatr)

model_robust <- lm_robust(net_indkomst ~ udd_aar + antal_timer_arb, data = df)

(texreg::screenreg(list(model_robust), include.ci=F,
                custom.coef.names = c("Konstantled",
                                        "Antal √•rs uddannelse",
                                        "Antal arbejdstimer")))
```
:::

------------------------------------------------------------------------

# Heteroskedasticitet

Man kan formelt teste for heteroskedasticitet med ***Breush Pagan Testen***. Den tager udgangspunkt i en regressionsmodel:\
$$y_i=\beta_0+\beta_1X_{i1}+\beta_2X_{i2}+\dots+\beta_kX_{ik}+u_i$$

Testen unders√∏ger, om variansen af $u_i$‚Äã afh√¶nger systematisk af de uafh√¶ngige variabler. Dette g√∏res ved at estimere en hj√¶lpe-regression:

$$u_i^2=\gamma_0+\gamma_1X_{i1}+\gamma_2X_{i2}+\dots+\gamma_kX_{ik}+v_i$$

hvor vi tester, om koefficienterne $\gamma_1, \gamma_2, ..., \gamma_k$‚Äã samlet set er signifikant forskellige fra nul.

---

# Heteroskedasticitet

Hypoteserne er som f√∏lgende:

**H0**: Der er *ingen* heteroskedasticitet

**H1**: Der *er* heteroskedasticitet

```{r echo=T, warning=F, message=F}
library(lmtest)

bptest_result <- bptest(model)

(bptest_summary <- data.frame(
  `Teststatistik` = bptest_result$statistic,       # Teststatistik
  `Frihedsgrader` = bptest_result$parameter,       # Frihedsgrader
  `P-v√¶rdi` = sprintf("%.4f", bptest_result$p.value) # P-v√¶rdi med 4 decimaler
))
```

P-v√¶rdien p√• mindre end 0,0000 viser, at vi kan afvise nulhypotesen og accepterer H1. Der er alts√• heteroskedasticitet tilstede.

---

# Autokorrelation

**5 Residualerne skal v√¶re uafh√¶ngigt fordelt (ingen autokorrelation)**

<br>

-   Autokorrelation beskriver en situation, hvor residualerne for forskellige observationer ikke er uafh√¶ngige

-   Dette kan f.eks. ske, hvis man laver klyngebaseret udv√¶lgelse til sin stikpr√∏ve p√• husholdsningsniveau blandt alle husholdniner i Gr√∏nland. Her opst√•r den udfordring, fordi folk i samme husstand typisk vil ligne hinanden p√• f.eks. hvor mange b√∏rn de har‚Ä¶

-   Afstanden mellem deres observerede indkomst og modellen forudsagte v√¶rdier kan derfor t√¶nkes at korrelere‚Ä¶ de er ikke l√¶ngere uafh√¶ngige.

------------------------------------------------------------------------

# Autokorrelation

<br>

-   Resultatet kan v√¶re, at man kommer til at undervurdere den statistiske usikkerhed

-   Det kan heldigvis let korrigeres i R ved at specificere sine clustre og lave ‚Äùclustrede standardfejl‚Äù

```{r echo=TRUE, warning=F, message=F, eval=F}
model_robust <- lm_robust(net_indkomst ~ udd_aar + antal_timer_arb, 
                          data = df,
                          clusters = husstand)
```

------------------------------------------------------------------------

# Omitted variable bias (OVB)

**6 Modellen er korrekt specificeret (dvs. alle relevante variable er med (OVB) og ingen irrelevante variable er med**

<br>

-   Omitted variable bias eller udeladt variabel bias betyder, at vi har udeladt relevante variable i vores regressionsmodel

-   Pr√∏v at t√¶nk tilbage‚Ä¶ hvorfor laver vi overhovedet line√¶r regression fremfor univariat eller bivariat analyse? Det er fordi vi gerne vil fors√∏ge at forklare hvorfor vi ser forskelle i f.eks. Indkomst, eller unders√∏ge om bestemte faktorer s√•som mere uddannelse har en effekt p√• indkomst!

-   Problemet med at bruge simpel bivariat analyse som t-test eller chi-i-anden test er, at vi ikke kan kontrollere for andre forhold, som kan spille ind samtidig‚Ä¶ men i regression kan vi bruge kontrolvariable!

------------------------------------------------------------------------

# Omitted variable bias (OVB)

Kontrolvariable indf√∏res i modellen, fordi vi mist√¶nker, at de kan v√¶re ‚Äùcounfounders‚Äù der kan forstyrre sammenh√¶ngen mellem den afh√¶ngige variabel og den uafh√¶ngige variabel, som vi gerne vil m√•le

![](images/reg5.png)

------------------------------------------------------------------------

# Omitted variable bias (OVB)

Hvis vi lykkedes med at inkludere ALLE counfounders som kontrolvariable, s√• har vi i princippet en kausal sammenh√¶ng tilbage mellem vores afh√¶ngige og uafh√¶ngige variabel

![](images/reg5.png)

Det ville nemlig betyde, at vi har isoleret den uforstyrrede sammenh√¶ng mellem X og Y

------------------------------------------------------------------------

# Omitted variable bias (OVB)

-   Dette er dog en **meget streng antagelse** at g√∏re sig

-   Det er s√• **godt som umuligt** at tage h√∏jde for alt med kontrolvariable, da ikke alt kan observeres og m√•les perfekt‚Ä¶ hvad med personlighed? Karakter? Gener? Grundl√¶ggende formning gennem barndommen?

-   **OVB er derfor snarere et vilk√•r** i den type af regressionsanalyse, som vi laver. Vi kan sj√¶ldent p√•st√• at have taget h√∏jde for alt

-   **Konsekvensen** er, at vi sandsynligvis over- eller undervurderer vores koefficienter

-   Vi kan stort set kun komme OVB til livs ved at arbejde med quasi-eksperimentelle metoder, hvor vi g√∏r os en hel r√¶kke andre antagelser...

------------------------------------------------------------------------

# Endogenitet

**7 Endogenitet**

-   OVB er t√¶t knyttet til antagelsen om endogenitet

-   Endogenitet betyder, at vi antager, at den uafh√¶ngige variabel (forklaringsvariablen) ikke er korreleret med fejlleddet i modellen

-   Dette betyder, at den variabel, som vi vil unders√∏ge virkningen af, ikke p√•virkes af andre faktorer, som ikke er inkluderet i modellen.

-   Hvis en uafh√¶ngig variabel korrelerer med fejlledet, s√• korrellerer den med noget ‚Äùuforklaret‚Äù

-   I bund og grund handler endogenitetsantagelsen i OLS om at sikre, at den uafh√¶ngige variabel er uafh√¶ngig af ukontrollerede faktorer for at opn√• p√•lidelige estimater af variablens virkning p√• den afh√¶ngige variabel
